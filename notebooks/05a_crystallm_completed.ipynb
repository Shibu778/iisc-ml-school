{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoriVfq4qVGU"
      },
      "source": [
        "# CrystaLLM Tutorial Practical\n",
        "\n",
        "In this practical we will download a pre-trained version of `CrystaLLM` we will use this to generate some potential crystal structures based on text prompts of the formula, and the formula with a spacegroup.\n",
        "\n",
        "![]( https://github.com/lantunes/CrystaLLM/blob/main/resources/crystallm-github.png)\n",
        "\n",
        "# Note - rememeber to enable GPU for this practical\n",
        "\n",
        "If you do not enable GPU, then later on you will need to specify to `CrystaLLM` to run on CPU. In the `RunCrystaLLM` cell, set `device=cpu`.\n",
        "\n",
        "## Set up the code\n",
        "\n",
        "We clone the git repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e0ZbJmjk6SA",
        "outputId": "7e93133e-3285-49cc-d0e2-8a029a06beb7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content')\n",
        "if not os.path.exists('CrystaLLM'):\n",
        "  !git clone https://github.com/lantunes/CrystaLLM.git\n",
        "\n",
        "os.chdir('CrystaLLM')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85fMKS4LSob0"
      },
      "source": [
        "Install the requirements for `CrystaLLM`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTFxLHNklaNL",
        "outputId": "fc18ac28-b018-49a5-9ee4-b069264920c2"
      },
      "outputs": [],
      "source": [
        "pip install -r requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vym7_nFra2k"
      },
      "source": [
        "Now put `CrystaLLM` on the PythonPath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LrPciFQmTec",
        "outputId": "9d1b73dd-e4d9-43a4-eb07-01437ee3f56e"
      },
      "outputs": [],
      "source": [
        "pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynirg4i5rqCP"
      },
      "source": [
        "## Obtain the pretrained model\n",
        "\n",
        "The pretrained `CrystaLLM` as published in XXXX is available to download from Zenodo. There is a helpful `bin/download.py` script to help you with this.\n",
        "\n",
        "We download the small model (~25M parameters). But you can also access the larger model using `!tar xvf crystallm_v1_large.tar.gz`. In addition there are other models fror download, which are trained on different datasets, for the full list see the [config directory](https://github.com/lantunes/CrystaLLM/tree/main/config) of the repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp0Tz03YoYH0",
        "outputId": "39bb5ed4-d6f3-4fb6-bffd-742cc17027c6"
      },
      "outputs": [],
      "source": [
        "!python bin/download.py crystallm_v1_small.tar.gz\n",
        "!tar xvf crystallm_v1_small.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elxmxmFws-P4"
      },
      "source": [
        "## Generate a prompt\n",
        "\n",
        "`CrystaLLM` needs a prompt to start generating a file. This prompt is the opening text of the file. At its simplest we can just give a chemical formula. We put the prompt into a `.txt` file that will be read by the run script later on. We could also add a spacegroup using the `--spacegroup` option:\n",
        "\n",
        "```\n",
        "python bin/make_prompt_file.py Na2Cl2 my_sg_prompt.txt --spacegroup P4/nmm\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mlhpz87SnoS7",
        "outputId": "9d68f1a1-b124-480a-c073-ccb297eb1c90"
      },
      "outputs": [],
      "source": [
        "!python bin/make_prompt_file.py Sb2Se3 my_prompt.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGxnhp9xt-zi"
      },
      "source": [
        "## Run `CrystaLLM`\n",
        "\n",
        "\n",
        "To randomly sample from a trained model, and generate CIF files, use the `bin/sample.py` script. The sampling script\n",
        "expects the path to the folder containing the trained model checkpoint, as well as the prompt, and other configuration\n",
        "options.\n",
        "\n",
        "<details>\n",
        "  <summary>Click for supported configuration options and their default values</summary>\n",
        "\n",
        "  ```python\n",
        "  out_dir: str = \"out\"  # the path to the directory containing the trained model\n",
        "  start: str = \"\\n\"  # the prompt; can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "  num_samples: int = 2  # number of samples to draw\n",
        "  max_new_tokens: int = 3000  # number of tokens generated in each sample\n",
        "  temperature: float = 0.8  # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "  top_k: int = 10  # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "  seed: int = 1337\n",
        "  device: str = \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "  dtype: str = \"bfloat16\"  # 'float32' or 'bfloat16' or 'float16'\n",
        "  compile: bool = False  # use PyTorch 2.0 to compile the model to be faster\n",
        "  target: str = \"console\"  # where the generated content will be sent; can also be 'file'\n",
        "  ```\n",
        "\n",
        "</details>\n",
        "\n",
        "For example:\n",
        "```shell\n",
        "python bin/sample.py \\\n",
        "out_dir=out/my_model \\\n",
        "start=FILE:my_prompt.txt \\\n",
        "num_samples=2 \\\n",
        "top_k=10 \\\n",
        "max_new_tokens=3000 \\\n",
        "device=cuda\n",
        "```\n",
        "In the above example, the trained model checkpoint file exists in the `out/my_model` directory. The prompt is\n",
        "in a file located at `my_prompt.txt`. Alternatively, we could also have placed the configuration options in a .yaml\n",
        "file, as we did for training, and specified its path using the `--config` command line option.\n",
        "\n",
        "Instead of specifying a file containing the prompt, we could also have specified the prompt directly:\n",
        "```shell\n",
        "python bin/sample.py \\\n",
        "out_dir=out/my_model \\\n",
        "start=$'data_Na2Cl2\\n' \\\n",
        "num_samples=2 \\\n",
        "top_k=10 \\\n",
        "max_new_tokens=3000 \\\n",
        "device=cuda\n",
        "```\n",
        "Assuming we're in a bash environment, we use the `$'string'` syntax for the `start` argument, since we'd like to\n",
        "specify the `\\n` (new line) character at the end of the prompt.\n",
        "\n",
        "The generated CIF files are sent to the console by default. Include the `target=file` argument to save the generated CIF\n",
        "files locally. (Each file will be named `sample_1.cif`, `sample_2.cif`, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqznF8QAn0ed",
        "outputId": "f34d558a-1aa5-4aef-be88-9b8341e6c2e6"
      },
      "outputs": [],
      "source": [
        "! python bin/sample.py \\\n",
        "out_dir=crystallm_v1_small/ \\\n",
        "start=FILE:my_prompt.txt \\\n",
        "num_samples=3 \\\n",
        "top_k=10 \\\n",
        "max_new_tokens=3000 \\\n",
        "device=cuda \\\n",
        "target=file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9W2NJWItxXD"
      },
      "source": [
        "## Visualise the results\n",
        "\n",
        "Use ASE to see what our outputs looked like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "LwjUfuuioM7-",
        "outputId": "a0b4b39f-fb3e-4188-9145-db5646d84950"
      },
      "outputs": [],
      "source": [
        "import ase.io\n",
        "from ase.visualize import view\n",
        "from ase.build import make_supercell\n",
        "import numpy as np\n",
        "\n",
        "structure = ase.io.read('sample_3.cif')\n",
        "supercell = 3\n",
        "print('Formula: ', structure.symbols)\n",
        "print('Unit cell: ', structure.cell)\n",
        "view(make_supercell(structure, supercell * np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])), viewer='x3d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-P5nvCV0oFy"
      },
      "source": [
        "## Save and download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bAle89Q91HRl"
      },
      "outputs": [],
      "source": [
        "structures = []\n",
        "for i in range(1,4):\n",
        "  structure = ase.io.read('sample_%s.cif'%str(i))\n",
        "  structures.append(structure)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "CCfW_Ea9Pk-U",
        "outputId": "d892bd6e-6fc9-4d3e-f9d0-8833e15d730b"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "for i, structure in enumerate(structures):\n",
        "    ase.io.write(f'structure_{i + 1}.cif', structure)\n",
        "    files.download(f'structure_{i + 1}.cif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "aQ2bRkHB0nIV",
        "outputId": "9dfe9951-076b-4dfb-952b-c751835cb9f4"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"llm-generated.pkl\", \"wb\") as f: # \"wb\" because we want to write in binary mode\n",
        "    pickle.dump(structures, f)\n",
        "\n",
        "\n",
        "files.download('llm-generated.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BIjvbUGuulE"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "* Try generating other structures\n",
        "* Try doing sample generation with spacegroup as well as composition\n",
        "* Load one of the other models - e.g. `crystallm_carbon_24_small` this has been trained only on allotropes of carbon. How good is this at generating a perovskite structure?\n",
        "* Try out the large model - do the results look different to the small model?\n",
        "* You can try to do generation using Monte Carlo Tree Search to choose conditioned next tokens, see [the documentation here](https://github.com/lantunes/CrystaLLM?tab=readme-ov-file#monte-carlo-tree-search-decoding) - in principle this should lead to lower energy genearated structures. See how it affects your generations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0Bo67YXutq8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HdE_X5DpJ3v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
