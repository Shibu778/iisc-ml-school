{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# helpers\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.style.use('sciml-style')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0\n",
    "\n",
    "To start with we set up our data and look at the dataset.\n",
    "\n",
    "During many of examples in these lectures we will work with the `fashion-mnist` dataset. This is useful for quick examples when learning the basics. When you come to the exercises you will use the same principles to work with more relevant datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### tool functions #####\n",
    "\n",
    "# plot an image in a subplot\n",
    "def subplot_image(image, label, nrows=1, ncols=1, iplot=0):\n",
    "    plt.subplot(nrows, ncols, iplot + 1)\n",
    "    plt.imshow(image, cmap=plt.cm.binary)\n",
    "    plt.xlabel(label)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "# plot a bar chart in a subplot\n",
    "def subplot_bar(data, true_label, nrows=1, ncols=1, iplot=0):\n",
    "    plt.subplot(nrows, ncols, iplot + 1)\n",
    "    chart = plt.bar(np.arange(len(data)), data, color='gray')\n",
    "    predicted_label = np.argmax(data)\n",
    "    chart[predicted_label].set_color('red')\n",
    "    chart[true_label].set_color('green')\n",
    "    plt.xticks(np.arange(len(data)))\n",
    "    plt.yticks([])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.gca().set_aspect(len(data))\n",
    "    plt.title('Probability', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# normalise images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# string labels\n",
    "string_labels = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# info\n",
    "print(\"number of training data: %d\" % len(train_labels))\n",
    "print(\"number of test data: %d\" % len(test_labels))\n",
    "print(\"image pixels: %s\" % str(train_images[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "nrows = 4\n",
    "ncols = 8\n",
    "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n",
    "for i in np.arange(nrows * ncols):\n",
    "    title = \"%d: %s\" % (train_labels[i], string_labels[train_labels[i]])\n",
    "    subplot_image(train_images[i], title, nrows, ncols, i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Building the network architecture\n",
    "\n",
    "Using the information and code snippits from the slides, we are now going to build a simple multi-layer perceptron to classify the `fashion-mnist` dataset. To build this network you will need\n",
    "\n",
    "* Input layer\n",
    "* Hidden layers\n",
    "* Output layers\n",
    "* All layers are `Dense` layers\n",
    "* Layers require activation functions too\n",
    "\n",
    "Note the we use different activation functions for different tasks. Remember that `ReLU` performs very well for training networks - there is a caveat, `ReLU` is *only* used in hidden layers. \n",
    "\n",
    "### The input layer\n",
    "\n",
    " You need to define the dimensionality of the input for the input layer. In this case we flatten (using a `Flatten` layer) the images above and feed them to the network. As they are $28 x 28$ images, the input size is 784.\n",
    "\n",
    "### The hidden layers\n",
    "\n",
    "For this network we will use just one hidden layer. It will be a `Dense` layer. This layer requires an activation function.\n",
    "\n",
    "### Output layers\n",
    "\n",
    "Remember from earliers lectures that we can cast categorical data as a `one-hot-vector`, in this case this is what we have. A vector of length 10, where each element corresponds to a type of apparel. We want our outputs to ideally be either 0 or 1, depending on class. If you rememeber our activation functions from the slides, this sounds like a `sigmoid` function\n",
    "\n",
    "$S(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at the summary of your model using `model.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary\n",
    "model.summary()\n",
    "# save initial weights\n",
    "dnn_initial_weights = model.get_weights().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Compile and train your model\n",
    "\n",
    "We start by compiling you model. This is where you specify all sorts of hyperparameters associated with the model. You need to choose an optimiser, recall from the slides we recommend *Adam* as a good generic choice. You also want an loss and some metrics.\n",
    "\n",
    "### Loss\n",
    "\n",
    "The loss is the value that is used to train the network, as in the slides, gradients of the loss with respect to weights are used to backpropagate and update the network weights. In this case we will use `SparseCategoricalCrossentropy` - the term *sparse* means that the output vector is sparse i.e. there are many more zeros than ones in the one hot encoding.\n",
    "\n",
    "### Metric\n",
    "\n",
    "The metrics do not feed into the training but are useful to monitor to get a sense of how the model is doing. You can also use them to choose between models at the end. In our case we will monitor the `accuracy`.\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam',\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer='adam',\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Train the model\n",
    "\n",
    "Now we can finally start to train the model on our data.\n",
    "\n",
    "The model is trained using the `fit` method. We need to specificy a few extra parameters here\n",
    "\n",
    "* Epochs: This is the number of times that the model will run through the entire training set during training\n",
    "* Batch size: This determines how many images will be used at a time to update the weights. Backpropagation uses the mean of the losses across the batch to update weights.  Generally larger batches will train quicker and smoother, but can become trapped in local minima. Generally batch sizes between 16 - 256 are used, depending on data heterogeniety, memory available etc ..\n",
    "* Validation split: we can automatically split the dataset into train and validate segments. Here we will use 20% for validation\n",
    "\n",
    "```\n",
    "training_history = model.fit(train_images, train_labels, epochs=20, batch_size=32, validation_split=0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = model.fit(train_images, train_labels, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results. Plot the accuracy and the validation accuracy against training epoch\n",
    "Notice that the values diverge as the model trains. This is a classic symptom of overfitting. We return to the slides to see how me might deal with this.\n",
    "```\n",
    "plt.plot(training_history.history['accuracy'])\n",
    "plt.plot(training_history.history['val_accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_history.history['accuracy'])\n",
    "plt.plot(training_history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Rgularise and re-train\n",
    "\n",
    "As we saw above, the model was doing okay, but was overfitting a bit. From the slides we learned that we can use `dropout` to mitigate against overfitting. Here we will rebuild our model and include some dropout between the hidden and output layer. We can see if this negates the overfitting.\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model_reg = Sequential()\n",
    "model_reg.add(Flatten(input_shape=(28, 28)))\n",
    "model_reg.add(Dense(128, activation='relu'))\n",
    "model_reg.add(Dropout(0.2))\n",
    "model_reg.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "```\n",
    "\n",
    "compile and fit the model as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model_reg = Sequential()\n",
    "model_reg.add(Flatten(input_shape=(28, 28)))\n",
    "model_reg.add(Dense(128, activation='relu'))\n",
    "model_reg.add(Dropout(0.4))\n",
    "model_reg.add(Dense(10, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model_reg.compile(optimizer='adam',\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=['accuracy'])\n",
    "training_history_reg = model_reg.fit(train_images, train_labels, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_history_reg.history['accuracy'])\n",
    "plt.plot(training_history_reg.history['val_accuracy'])\n",
    "plt.plot(training_history.history['val_accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
