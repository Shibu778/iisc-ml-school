{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using machine learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concepts covered\n",
    "\n",
    "* Supervised machine learning\n",
    "* Classical machine learning\n",
    "* Parameters/hyperparameters\n",
    "* Decision trees\n",
    "* Overfitting\n",
    "* Evaluation/metrics\n",
    "* Test/train split, cross-validation\n",
    "* Bagging and boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "plt.style.use('sciml-style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_classifier(model, ax=None, cmap='viridis'):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=20, cmap=cmap,\n",
    "               clim=(y.min(), y.max()), zorder=3, alpha=0.7, edgecolors='black' )\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # fit the estimator\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap, clim=(y.min(), y.max()),\n",
    "                           zorder=1)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Set up the data\n",
    "\n",
    "For this lecture and example we will be using a dataset of `blobs` that is generated automatically by `scikit-learn`. We generate a dataset of 300 samples with 4 different centres of the data. Use the code below to generate and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=1.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=20, alpha=0.8, edgecolors='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 setting up a decision tree\n",
    "\n",
    "We can import a decision tree classifier from `scikit-learn` and use this to try to classify the data into clsuters.\n",
    "\n",
    "Go to [lecture notes]() to cover the theory of decision trees\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "There are many hyperparameters that can be set for the decision tree classifier. For the purposes of this tutorial we will consider only a few of these. For a full list see [the `DecisionTreeClassifier` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "\n",
    "#### `max_depth`\n",
    "\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain fewer than `min_samples_split` samples.\n",
    "\n",
    "#### `min_samples_split`\n",
    "\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "    * If int, then consider min_samples_split as the minimum number.\n",
    "    * If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "    \n",
    "#### `max_features`\n",
    "\n",
    "The maximum number of features of the input data that can be used for splitting. If this is left as `None` then the `max_features` is the number of features.\n",
    "\n",
    "#### Our model\n",
    "\n",
    "For our model we will simply go with the default hyperparameters and see how the network performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit the model to our data\n",
    "\n",
    "In this instance we do not need to do much feature engineering - the data that we have are relatively simple and have only two features; the coordinates on the plane. \n",
    "\n",
    "The model objects in `scikit-learn` have a `fit` method, which takes the feature `X` and label `y` data as inputs and fits the model. Use the code below to fit the model and then use the `visualize_classifier` helper function to look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = decision_tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the middle there seem to be rather arbitrary classification of regions of the data. Try rerunnig the clustering for different random sub-sets of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "decision_tree = tree.fit(X[::2], y[::2])\n",
    "visualize_classifier(tree, ax=ax[0])\n",
    "decision_tree = tree.fit(X[1::2], y[1::2])\n",
    "visualize_classifier(tree, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the classification is consistent in some regions, but quite different in the centre of the plot. It is concerning that the details of the classification in the centre of the plot are depedent on the training data. This is a classic sign that the model is **overfitting**.\n",
    "\n",
    "Of course, we can see visually that this overfitting is occuring, but is there a more systematic way of detecting overfitting?\n",
    "\n",
    "To do this we first need to have an objective measure of how well the model is performing. We need a **metric**.\n",
    "\n",
    "Go to [lecture notes]() to cover metrics for evaluation of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluation\n",
    "\n",
    "We are now in a position to evaluate how the model is performing. Let's start by once again fitting the model to our dataset `(X, y)`. \n",
    "\n",
    "Next we get the predicted values of `y` by using the `predict` method of the new model.\n",
    "\n",
    "Finally use the `accuracy_score` metric from `scikit-learn` to get the accuracy of the model on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree = DecisionTreeClassifier().fit(X, y)\n",
    "y_pred = tree.predict(X)\n",
    "print(accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty accurate!!\n",
    "\n",
    "But wait up a moment - let's try the model on some new data - generated the same way as before, but which was not used in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew, ynew = make_blobs(n_samples=100, centers=4,\n",
    "                  random_state=0, cluster_std=1.6)\n",
    "plt.scatter(Xnew[:, 0], Xnew[:, 1], marker='o', edgecolors='black', c=ynew, s=20, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(Xnew)\n",
    "print(accuracy_score(ynew, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not quite as good as before!\n",
    "\n",
    "The model is overfitting - i.e. it is performing well on training data but not generalising beyond the training set. This often results when the model has too many *parameters* and fits too flexibly to the training data. The number of parameters in the model is a *hyper parameter* and must be specified to choose the model. \n",
    "\n",
    "## Step 4: Hyperparameter tuning and crossvalidation\n",
    "\n",
    "We are going to use a training/test split and cross-validation to tune the hyper parameters of the model. Go to the [lecture notes]() for some more in-depth examples.\n",
    "\n",
    "Below is some code to set up a search across the `max_depth` parameter ranging from 1 - 20. For each value of the hyperparameter, we use a 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'max_depth':range(1, 20, 1)}\n",
    "tree = DecisionTreeClassifier()\n",
    "search = GridSearchCV(tree, param_grid, cv=5)\n",
    "gs = search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(search.cv_results_['mean_test_score'])\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('Val Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the cross-validation score has a maximum at somewhere around 3 for the `max_depth`.\n",
    "We take the best performing hyperparameter and retrain the model on the full dataset, we than apply this model to the new, unseen data and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = gs.best_params_['max_depth']\n",
    "tree = DecisionTreeClassifier(max_depth=depth).fit(X, y)\n",
    "y_pred = tree.predict(Xnew)\n",
    "print(accuracy_score(ynew, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Bagging and boosting\n",
    "\n",
    "We can improve the performance of decision trees in two simple ways. Bagging and boosting. Bagging involves training an ensemble of trees on subsets of the training set. This way we can take the average result of the ensemble rather than any given individual result.\n",
    "\n",
    "Boosting involves training a new decision tree based on the errors of the first tree. See the [lecture notes]() for a more in depth discussion of bagging and boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=300, random_state=0)\n",
    "model = forest.fit(X, y)\n",
    "visualize_classifier(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest.predict(Xnew)\n",
    "print(accuracy_score(ynew, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Try to do hyperparameter tuning on the boosted decision tree to see if you can improve this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
